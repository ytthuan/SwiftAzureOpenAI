// Generated Swift Models from OpenAPI Specification
// DO NOT EDIT: This file is automatically generated

import Foundation

/// Generated enum for AzureAIFoundryModelsApiVersion
public enum GeneratedAIFoundryModelsApiVersion: String, Codable, CaseIterable {
    case v1 = "v1"
    case preview = "preview"
}

/// Generated enum for AzureFileExpiryAnchor
public enum GeneratedFileExpiryAnchor: String, Codable, CaseIterable {
    case created_at = "created_at"
}

/// Generated enum for OpenAI.Includable
public enum GeneratedIncludable: String, Codable, CaseIterable {
    case code_interpreter_call_outputs = "code_interpreter_call.outputs"
    case computer_call_output_output_image_url = "computer_call_output.output.image_url"
    case file_search_call_results = "file_search_call.results"
    case message_input_image_image_url = "message.input_image.image_url"
    case message_output_text_logprobs = "message.output_text.logprobs"
    case reasoning_encrypted_content = "reasoning.encrypted_content"
}

/// Generated enum for OpenAI.ItemContentType
public enum GeneratedItemContentType: String, Codable, CaseIterable {
    case input_text = "input_text"
    case input_audio = "input_audio"
    case input_image = "input_image"
    case input_file = "input_file"
    case output_text = "output_text"
    case output_audio = "output_audio"
    case refusal = "refusal"
}

/// Generated enum for OpenAI.ItemType
public enum GeneratedItemType: String, Codable, CaseIterable {
    case message = "message"
    case file_search_call = "file_search_call"
    case function_call = "function_call"
    case function_call_output = "function_call_output"
    case computer_call = "computer_call"
    case computer_call_output = "computer_call_output"
    case web_search_call = "web_search_call"
    case reasoning = "reasoning"
    case item_reference = "item_reference"
    case image_generation_call = "image_generation_call"
    case code_interpreter_call = "code_interpreter_call"
    case local_shell_call = "local_shell_call"
    case local_shell_call_output = "local_shell_call_output"
    case mcp_list_tools = "mcp_list_tools"
    case mcp_approval_request = "mcp_approval_request"
    case mcp_approval_response = "mcp_approval_response"
    case mcp_call = "mcp_call"
}

/// Generated enum for OpenAI.ReasoningEffort
public enum GeneratedReasoningEffort: String, Codable, CaseIterable {
    case low = "low"
    case medium = "medium"
    case high = "high"
}

/// Generated enum for OpenAI.ResponseErrorCode
public enum GeneratedResponseErrorCode: String, Codable, CaseIterable {
    case server_error = "server_error"
    case rate_limit_exceeded = "rate_limit_exceeded"
    case invalid_prompt = "invalid_prompt"
    case vector_store_timeout = "vector_store_timeout"
    case invalid_image = "invalid_image"
    case invalid_image_format = "invalid_image_format"
    case invalid_base64_image = "invalid_base64_image"
    case invalid_image_url = "invalid_image_url"
    case image_too_large = "image_too_large"
    case image_too_small = "image_too_small"
    case image_parse_error = "image_parse_error"
    case image_content_policy_violation = "image_content_policy_violation"
    case invalid_image_mode = "invalid_image_mode"
    case image_file_too_large = "image_file_too_large"
    case unsupported_image_media_type = "unsupported_image_media_type"
    case empty_image_file = "empty_image_file"
    case failed_to_download_image = "failed_to_download_image"
    case image_file_not_found = "image_file_not_found"
}

/// Generated enum for OpenAI.ToolChoiceOptions
public enum GeneratedToolChoiceOptions: String, Codable, CaseIterable {
    case none = "none"
    case auto = "auto"
    case required = "required"
}

/// Generated model for AzureCreateEmbeddingRequest
public struct GeneratedEmbeddingRequest: Codable, Equatable {
    /// The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3` and later models.
    public let dimensions: Int?

    /// The format to return the embeddings in. Can be either `float` or `base64`.
    public let encodingFormat: String?

    /// Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input...
    public let input: String

    /// The model to use for the embedding request.
    public let model: String

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.
    public let user: String?

    private enum CodingKeys: String, CodingKey {
        case dimensions
        case encodingFormat = "encoding_format"
        case input
        case model
        case user
    }

}

/// Generated model for AzureCreateFileRequestMultiPart
public struct GeneratedFileRequestMultiPart: Codable, Equatable {
    public let expiresAfter: SAOAIJSONValue

    public let file: String

    /// The intended purpose of the uploaded file. One of: - `assistants`: Used in the Assistants API - `batch`: Used in the Batch API - `fine-tune`: Used for fine-tuning - `evals`: Used for eval data sets
    public let purpose: String

    private enum CodingKeys: String, CodingKey {
        case expiresAfter = "expires_after"
        case file
        case purpose
    }

}

/// Generated model for AzureCreateResponse
public struct GeneratedResponseRequest: Codable, Equatable {
    /// Whether to run the model response in the background. Learn more.
    public let background: Bool?

    /// Specify additional output data to include in the model response. Currently supported values are: Includes the outputs of python code execution in code interpreter tool call items. Include image url...
    public let include: [GeneratedIncludable]?

    /// Text, image, or file inputs to the model, used to generate a response. Learn more: - Text inputs and outputs - Image inputs - File inputs - Conversation state - Function calling
    public let input: String?

    /// A system (or developer) message inserted into the model's context. When using along with `previous_response_id`, the instructions from a previous response will not be carried over to the next respo...
    public let instructions: String?

    /// An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
    public let maxOutputTokens: Int?

    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to...
    public let maxToolCalls: Int?

    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the d...
    public let metadata: SAOAIJSONValue?

    /// The model deployment to use for the creation of this response.
    public let model: String

    /// Whether to allow the model to run tool calls in parallel.
    public let parallelToolCalls: Bool?

    /// The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about conversation state.
    public let previousResponseId: String?

    public let prompt: SAOAIJSONValue?

    public let reasoning: SAOAIJSONValue?

    /// Whether to store the generated model response for later retrieval via API.
    public let store: Bool?

    /// If set to true, the model response data will be streamed to the client as it is generated using server-sent events. See the Streaming section below for more information.
    public let stream: Bool?

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally rec...
    public let temperature: Float?

    /// Configuration options for a text response from the model. Can be plain text or structured JSON data. Learn more: - Text inputs and outputs - Structured Outputs
    public let text: SAOAIJSONValue?

    /// How the model should select which tool (or tools) to use when generating a response. See the `tools` parameter to see how to specify which tools the model can call.
    public let toolChoice: String?

    /// An array of tools the model may call while generating a response. You can specify which tool to use by setting the `tool_choice` parameter. The two categories of tools you can provide the model are...
    public let tools: [GeneratedTool]?

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    public let topLogprobs: Int?

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the to...
    public let topP: Float?

    /// The truncation strategy to use for the model response. If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the con...
    public let truncation: String?

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.
    public let user: String?

    private enum CodingKeys: String, CodingKey {
        case background
        case include
        case input
        case instructions
        case maxOutputTokens = "max_output_tokens"
        case maxToolCalls = "max_tool_calls"
        case metadata
        case model
        case parallelToolCalls = "parallel_tool_calls"
        case previousResponseId = "previous_response_id"
        case prompt
        case reasoning
        case store
        case stream
        case temperature
        case text
        case toolChoice = "tool_choice"
        case tools
        case topLogprobs = "top_logprobs"
        case topP = "top_p"
        case truncation
        case user
    }

}

/// Generated model for AzureErrorResponse
public struct GeneratedErrorResponse: Codable, Equatable {
    /// The error details.
    public let error: SAOAIJSONValue?

}

/// Generated model for AzureListFilesResponse
public struct GeneratedListFilesResponse: Codable, Equatable {
    public let data: [GeneratedOpenAIFile]

    public let firstId: String

    public let hasMore: Bool

    public let lastId: String

    public let object: String

    private enum CodingKeys: String, CodingKey {
        case data
        case firstId = "first_id"
        case hasMore = "has_more"
        case lastId = "last_id"
        case object
    }

}

/// Generated model for AzureOpenAIFile
public struct GeneratedOpenAIFile: Codable, Equatable {
    /// The size of the file, in bytes.
    public let bytes: Int64

    /// The Unix timestamp (in seconds) for when the file was created.
    public let createdAt: Int

    /// The Unix timestamp (in seconds) for when the file will expire.
    public let expiresAt: Int?

    /// The name of the file.
    public let filename: String

    /// The file identifier, which can be referenced in the API endpoints.
    public let id: String

    /// The object type, which is always `file`.
    public let object: String

    /// The intended purpose of the file. Supported values are `assistants`, `assistants_output`, `batch`, `batch_output`, `fine-tune` and `fine-tune-results`.
    public let purpose: String

    public let status: String

    /// Deprecated. For details on why a fine-tuning training file failed validation, see the `error` field on `fine_tuning.job`.
    public let statusDetails: String?

    private enum CodingKeys: String, CodingKey {
        case bytes
        case createdAt = "created_at"
        case expiresAt = "expires_at"
        case filename
        case id
        case object
        case purpose
        case status
        case statusDetails = "status_details"
    }

}

/// Generated model for AzureResponse
public struct GeneratedResponse: Codable, Equatable {
    /// Whether to run the model response in the background. Learn more.
    public let background: Bool?

    /// Unix timestamp (in seconds) of when this Response was created.
    public let createdAt: Int

    public let error: SAOAIJSONValue

    /// Unique identifier for this Response.
    public let id: String

    /// Details about why the response is incomplete.
    public let incompleteDetails: SAOAIJSONValue

    /// A system (or developer) message inserted into the model's context. When using along with `previous_response_id`, the instructions from a previous response will not be carried over to the next respo...
    public let instructions: String

    /// An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
    public let maxOutputTokens: Int?

    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to...
    public let maxToolCalls: Int?

    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the d...
    public let metadata: SAOAIJSONValue

    /// The model used to generate this response.
    public let model: String

    /// The object type of this resource - always set to `response`.
    public let object: String

    /// An array of content items generated by the model. - The length and order of items in the `output` array is dependent on the model's response. - Rather than accessing the first item in the `output` ...
    public let output: [GeneratedItemResource]

    /// SDK-only convenience property that contains the aggregated text output from all `output_text` items in the `output` array, if any are present. Supported in the Python and JavaScript SDKs.
    public let outputText: String?

    /// Whether to allow the model to run tool calls in parallel.
    public let parallelToolCalls: Bool

    /// The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about conversation state.
    public let previousResponseId: String?

    public let prompt: SAOAIJSONValue?

    public let reasoning: SAOAIJSONValue?

    /// The status of the response generation. One of `completed`, `failed`, `in_progress`, `cancelled`, `queued`, or `incomplete`.
    public let status: String?

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally rec...
    public let temperature: Float

    /// Configuration options for a text response from the model. Can be plain text or structured JSON data. Learn more: - Text inputs and outputs - Structured Outputs
    public let text: SAOAIJSONValue?

    /// How the model should select which tool (or tools) to use when generating a response. See the `tools` parameter to see how to specify which tools the model can call.
    public let toolChoice: String?

    /// An array of tools the model may call while generating a response. You can specify which tool to use by setting the `tool_choice` parameter. The two categories of tools you can provide the model are...
    public let tools: [GeneratedTool]?

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    public let topLogprobs: Int?

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the to...
    public let topP: Float

    /// The truncation strategy to use for the model response. If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the con...
    public let truncation: String?

    public let usage: GeneratedResponseUsage?

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.
    public let user: String

    private enum CodingKeys: String, CodingKey {
        case background
        case createdAt = "created_at"
        case error
        case id
        case incompleteDetails = "incomplete_details"
        case instructions
        case maxOutputTokens = "max_output_tokens"
        case maxToolCalls = "max_tool_calls"
        case metadata
        case model
        case object
        case output
        case outputText = "output_text"
        case parallelToolCalls = "parallel_tool_calls"
        case previousResponseId = "previous_response_id"
        case prompt
        case reasoning
        case status
        case temperature
        case text
        case toolChoice = "tool_choice"
        case tools
        case topLogprobs = "top_logprobs"
        case topP = "top_p"
        case truncation
        case usage
        case user
    }

}

/// Generated model for OpenAI.CreateEmbeddingResponse
public struct GeneratedEmbeddingResponse: Codable, Equatable {
    /// The list of embeddings generated by the model.
    public let data: [GeneratedEmbedding]

    /// The name of the model used to generate the embedding.
    public let model: String

    /// The object type, which is always "list".
    public let object: String

    /// The usage information for the request.
    public let usage: SAOAIJSONValue

}

/// Generated model for OpenAI.DeleteFileResponse
public struct GeneratedDeleteFileResponse: Codable, Equatable {
    public let deleted: Bool

    public let id: String

    public let object: String

}

/// Generated model for OpenAI.Embedding
public struct GeneratedEmbedding: Codable, Equatable {
    /// The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the embedding guide.
    public let embedding: String

    /// The index of the embedding in the list of embeddings.
    public let index: Int

    /// The object type, which is always "embedding".
    public let object: String

}

/// Generated model for OpenAI.ImplicitUserMessage
public struct GeneratedImplicitUserMessage: Codable, Equatable {
    public let content: String

}

/// Generated model for OpenAI.ItemContent
public struct GeneratedItemContent: Codable, Equatable {
    public let type: GeneratedItemContentType

}

/// Generated model for OpenAI.ItemParam
public struct GeneratedItemParam: Codable, Equatable {
    public let type: GeneratedItemType

}

/// Generated model for OpenAI.ItemResource
public struct GeneratedItemResource: Codable, Equatable {
    public let id: String

    public let type: GeneratedItemType

}

/// Generated model for OpenAI.Prompt
public struct GeneratedPrompt: Codable, Equatable {
    /// The unique identifier of the prompt template to use.
    public let id: String

    public let variables: SAOAIJSONValue?

    /// Optional version of the prompt template.
    public let version: String?

}

/// Generated model for OpenAI.Reasoning
public struct GeneratedReasoning: Codable, Equatable {
    public let effort: String?

    /// **Deprecated:** use `summary` instead. A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of `auto`, `concise`,...
    public let generateSummary: String?

    /// A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of `auto`, `concise`, or `detailed`.
    public let summary: String?

    private enum CodingKeys: String, CodingKey {
        case effort
        case generateSummary = "generate_summary"
        case summary
    }

}

/// Generated model for OpenAI.ResponseError
public struct GeneratedResponseError: Codable, Equatable {
    public let code: GeneratedResponseErrorCode

    /// A human-readable description of the error.
    public let message: String

}

/// Generated model for OpenAI.ResponseItemList
public struct GeneratedResponseItemList: Codable, Equatable {
    /// A list of items used to generate this response.
    public let data: [GeneratedItemResource]

    /// The ID of the first item in the list.
    public let firstId: String

    /// Whether there are more items available.
    public let hasMore: Bool

    /// The ID of the last item in the list.
    public let lastId: String

    /// The type of object returned, must be `list`.
    public let object: String

    private enum CodingKeys: String, CodingKey {
        case data
        case firstId = "first_id"
        case hasMore = "has_more"
        case lastId = "last_id"
        case object
    }

}

/// Generated model for OpenAI.ResponsePromptVariables
public struct GeneratedResponsePromptVariables: Codable, Equatable {
}

/// Generated model for OpenAI.ResponseStreamEvent
public struct GeneratedResponseStreamEvent: Codable, Equatable {
    /// The sequence number for this event.
    public let sequenceNumber: Int

    public let type: String

    private enum CodingKeys: String, CodingKey {
        case sequenceNumber = "sequence_number"
        case type
    }

}

/// Generated model for OpenAI.ResponseTextFormatConfiguration
public struct GeneratedResponseTextFormatConfiguration: Codable, Equatable {
    public let type: String

}

/// Generated model for OpenAI.ResponseUsage
public struct GeneratedResponseUsage: Codable, Equatable {
    /// The number of input tokens.
    public let inputTokens: Int

    /// A detailed breakdown of the input tokens.
    public let inputTokensDetails: SAOAIJSONValue

    /// The number of output tokens.
    public let outputTokens: Int

    /// A detailed breakdown of the output tokens.
    public let outputTokensDetails: SAOAIJSONValue

    /// The total number of tokens used.
    public let totalTokens: Int

    private enum CodingKeys: String, CodingKey {
        case inputTokens = "input_tokens"
        case inputTokensDetails = "input_tokens_details"
        case outputTokens = "output_tokens"
        case outputTokensDetails = "output_tokens_details"
        case totalTokens = "total_tokens"
    }

}

/// Generated model for OpenAI.Tool
public struct GeneratedTool: Codable, Equatable {
    public let type: String

}

/// Generated model for OpenAI.ToolChoiceObject
public struct GeneratedToolChoiceObject: Codable, Equatable {
    public let type: String

}
