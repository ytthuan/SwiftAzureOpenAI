// Generated Swift Models from OpenAPI Specification
// DO NOT EDIT: This file is automatically generated

import Foundation

/// Generated enum for AzureAIFoundryModelsApiVersion
public enum GeneratedAIFoundryModelsApiVersion: String, Codable, CaseIterable {
    case v1 = "v1"
    case preview = "preview"
}

/// Generated enum for AzureFileExpiryAnchor
public enum GeneratedFileExpiryAnchor: String, Codable, CaseIterable {
    case createdat = "created_at"
}

/// Generated enum for OpenAI.Includable
public enum GeneratedIncludable: String, Codable, CaseIterable {
    case codeinterpretercall.outputs = "code_interpreter_call.outputs"
    case computercalloutput.output.imageurl = "computer_call_output.output.image_url"
    case filesearchcall.results = "file_search_call.results"
    case message.inputimage.imageurl = "message.input_image.image_url"
    case message.outputtext.logprobs = "message.output_text.logprobs"
    case reasoning.encryptedcontent = "reasoning.encrypted_content"
}

/// Generated enum for OpenAI.ItemContentType
public enum GeneratedItemContentType: String, Codable, CaseIterable {
    case inputtext = "input_text"
    case inputaudio = "input_audio"
    case inputimage = "input_image"
    case inputfile = "input_file"
    case outputtext = "output_text"
    case outputaudio = "output_audio"
    case refusal = "refusal"
}

/// Generated enum for OpenAI.ItemType
public enum GeneratedItemType: String, Codable, CaseIterable {
    case message = "message"
    case filesearchcall = "file_search_call"
    case functioncall = "function_call"
    case functioncalloutput = "function_call_output"
    case computercall = "computer_call"
    case computercalloutput = "computer_call_output"
    case websearchcall = "web_search_call"
    case reasoning = "reasoning"
    case itemreference = "item_reference"
    case imagegenerationcall = "image_generation_call"
    case codeinterpretercall = "code_interpreter_call"
    case localshellcall = "local_shell_call"
    case localshellcalloutput = "local_shell_call_output"
    case mcplisttools = "mcp_list_tools"
    case mcpapprovalrequest = "mcp_approval_request"
    case mcpapprovalresponse = "mcp_approval_response"
    case mcpcall = "mcp_call"
}

/// Generated enum for OpenAI.ReasoningEffort
public enum GeneratedReasoningEffort: String, Codable, CaseIterable {
    case low = "low"
    case medium = "medium"
    case high = "high"
}

/// Generated enum for OpenAI.ResponseErrorCode
public enum GeneratedResponseErrorCode: String, Codable, CaseIterable {
    case servererror = "server_error"
    case ratelimitexceeded = "rate_limit_exceeded"
    case invalidprompt = "invalid_prompt"
    case vectorstoretimeout = "vector_store_timeout"
    case invalidimage = "invalid_image"
    case invalidimageformat = "invalid_image_format"
    case invalidbase64image = "invalid_base64_image"
    case invalidimageurl = "invalid_image_url"
    case imagetoolarge = "image_too_large"
    case imagetoosmall = "image_too_small"
    case imageparseerror = "image_parse_error"
    case imagecontentpolicyviolation = "image_content_policy_violation"
    case invalidimagemode = "invalid_image_mode"
    case imagefiletoolarge = "image_file_too_large"
    case unsupportedimagemediatype = "unsupported_image_media_type"
    case emptyimagefile = "empty_image_file"
    case failedtodownloadimage = "failed_to_download_image"
    case imagefilenotfound = "image_file_not_found"
}

/// Generated enum for OpenAI.ToolChoiceOptions
public enum GeneratedToolChoiceOptions: String, Codable, CaseIterable {
    case none = "none"
    case auto = "auto"
    case required = "required"
}

/// Generated model for AzureCreateEmbeddingRequest
public struct GeneratedEmbeddingRequest: Codable, Equatable {
    /// The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3` and later models.
    public let dimensions: Int?

    /// The format to return the embeddings in. Can be either `float` or [`base64`](https://pypi.org/project/pybase64/).
    public let encodingFormat: String?

    /// Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for all embedding models), cannot be an empty string, and any array must be 2048 dimensions or less. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. In addition to the per-input token limit, all embedding  models enforce a maximum of 300,000 tokens summed across all inputs in a  single request.
    public let input: String

    /// The model to use for the embedding request.
    public let model: String

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
    public let user: String?

    private enum CodingKeys: String, CodingKey {
        case dimensions
        case encodingFormat = "encoding_format"
        case input
        case model
        case user
    }

}

/// Generated model for AzureCreateFileRequestMultiPart
public struct GeneratedFileRequestMultiPart: Codable, Equatable {
    public let expiresAfter: Generatedexpires_after

    public let file: String

    /// The intended purpose of the uploaded file. One of: - `assistants`: Used in the Assistants API - `batch`: Used in the Batch API - `fine-tune`: Used for fine-tuning - `evals`: Used for eval data sets
    public let purpose: String

    private enum CodingKeys: String, CodingKey {
        case expiresAfter = "expires_after"
        case file
        case purpose
    }

}

/// Generated model for AzureCreateResponse
public struct GeneratedResponseRequest: Codable, Equatable {
    /// Whether to run the model response in the background.
[Learn more](/docs/guides/background).
    public let background: Bool?

    /// Specify additional output data to include in the model response. Currently
supported values are:
- `code_interpreter_call.outputs`: Includes the outputs of python code execution
  in code interpreter tool call items.
- `computer_call_output.output.image_url`: Include image urls from the computer call output.
- `file_search_call.results`: Include the search results of
  the file search tool call.
- `message.input_image.image_url`: Include image urls from the input message.
- `message.output_text.logprobs`: Include logprobs with assistant messages.
- `reasoning.encrypted_content`: Includes an encrypted version of reasoning
  tokens in reasoning item outputs. This enables reasoning items to be used in
  multi-turn conversations when using the Responses API statelessly (like
  when the `store` parameter is set to `false`, or when an organization is
  enrolled in the zero data retention program).
    public let include: [GeneratedIncludable]?

    /// Text, image, or file inputs to the model, used to generate a response.

Learn more:
- [Text inputs and outputs](/docs/guides/text)
- [Image inputs](/docs/guides/images)
- [File inputs](/docs/guides/pdf-files)
- [Conversation state](/docs/guides/conversation-state)
- [Function calling](/docs/guides/function-calling)
    public let input: String?

    /// A system (or developer) message inserted into the model's context.

When using along with `previous_response_id`, the instructions from a previous
response will not be carried over to the next response. This makes it simple
to swap out system (or developer) messages in new responses.
    public let instructions: String?

    /// An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
    public let maxOutputTokens: Int?

    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
    public let maxToolCalls: Int?

    /// Set of 16 key-value pairs that can be attached to an object. This can be
useful for storing additional information about the object in a structured
format, and querying for objects via API or the dashboard.

Keys are strings with a maximum length of 64 characters. Values are strings
with a maximum length of 512 characters.
    public let metadata: [String: Any]?

    /// The model deployment to use for the creation of this response.
    public let model: String

    /// Whether to allow the model to run tool calls in parallel.
    public let parallelToolCalls: Bool?

    /// The unique ID of the previous response to the model. Use this to
create multi-turn conversations. Learn more about
[conversation state](/docs/guides/conversation-state).
    public let previousResponseId: String?

    public let prompt: [String: Any]?

    public let reasoning: [String: Any]?

    /// Whether to store the generated model response for later retrieval via
API.
    public let store: Bool?

    /// If set to true, the model response data will be streamed to the client
as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
See the [Streaming section below](/docs/api-reference/responses-streaming)
for more information.
    public let stream: Bool?

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
We generally recommend altering this or `top_p` but not both.
    public let temperature: Float?

    /// Configuration options for a text response from the model. Can be plain
text or structured JSON data. Learn more:
- [Text inputs and outputs](/docs/guides/text)
- [Structured Outputs](/docs/guides/structured-outputs)
    public let text: Generatedtext?

    /// How the model should select which tool (or tools) to use when generating
a response. See the `tools` parameter to see how to specify which tools
the model can call.
    public let toolChoice: String?

    /// An array of tools the model may call while generating a response. You 
can specify which tool to use by setting the `tool_choice` parameter.

The two categories of tools you can provide the model are:

- **Built-in tools**: Tools that are provided by OpenAI that extend the
  model's capabilities, like file search.
- **Function calls (custom tools)**: Functions that are defined by you,
  enabling the model to call your own code.
    public let tools: [GeneratedTool]?

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    public let topLogprobs: Int?

    /// An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability
mass. So 0.1 means only the tokens comprising the top 10% probability mass
are considered.

We generally recommend altering this or `temperature` but not both.
    public let topP: Float?

    /// The truncation strategy to use for the model response.
- `auto`: If the context of this response and previous ones exceeds
  the model's context window size, the model will truncate the
  response to fit the context window by dropping input items in the
  middle of the conversation.
- `disabled` (default): If a model response will exceed the context window
  size for a model, the request will fail with a 400 error.
    public let truncation: String?

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
    public let user: String?

    private enum CodingKeys: String, CodingKey {
        case background
        case include
        case input
        case instructions
        case maxOutputTokens = "max_output_tokens"
        case maxToolCalls = "max_tool_calls"
        case metadata
        case model
        case parallelToolCalls = "parallel_tool_calls"
        case previousResponseId = "previous_response_id"
        case prompt
        case reasoning
        case store
        case stream
        case temperature
        case text
        case toolChoice = "tool_choice"
        case tools
        case topLogprobs = "top_logprobs"
        case topP = "top_p"
        case truncation
        case user
    }

}

/// Generated model for AzureErrorResponse
public struct GeneratedErrorResponse: Codable, Equatable {
    /// The error details.
    public let error: Generatederror?

}

/// Generated model for AzureListFilesResponse
public struct GeneratedListFilesResponse: Codable, Equatable {
    public let data: [GeneratedOpenAIFile]

    public let firstId: String

    public let hasMore: Bool

    public let lastId: String

    public let object: String

    private enum CodingKeys: String, CodingKey {
        case data
        case firstId = "first_id"
        case hasMore = "has_more"
        case lastId = "last_id"
        case object
    }

}

/// Generated model for AzureOpenAIFile
public struct GeneratedOpenAIFile: Codable, Equatable {
    /// The size of the file, in bytes.
    public let bytes: Int64

    /// The Unix timestamp (in seconds) for when the file was created.
    public let createdAt: Int

    /// The Unix timestamp (in seconds) for when the file will expire.
    public let expiresAt: Int?

    /// The name of the file.
    public let filename: String

    /// The file identifier, which can be referenced in the API endpoints.
    public let id: String

    /// The object type, which is always `file`.
    public let object: String

    /// The intended purpose of the file. Supported values are `assistants`, `assistants_output`, `batch`, `batch_output`, `fine-tune` and `fine-tune-results`.
    public let purpose: String

    public let status: String

    /// Deprecated. For details on why a fine-tuning training file failed validation, see the `error` field on `fine_tuning.job`.
    public let statusDetails: String?

    private enum CodingKeys: String, CodingKey {
        case bytes
        case createdAt = "created_at"
        case expiresAt = "expires_at"
        case filename
        case id
        case object
        case purpose
        case status
        case statusDetails = "status_details"
    }

}

/// Generated model for AzureResponse
public struct GeneratedResponse: Codable, Equatable {
    /// Whether to run the model response in the background.
[Learn more](/docs/guides/background).
    public let background: Bool?

    /// Unix timestamp (in seconds) of when this Response was created.
    public let createdAt: Int

    public let error: [String: Any]

    /// Unique identifier for this Response.
    public let id: String

    /// Details about why the response is incomplete.
    public let incompleteDetails: Generatedincomplete_details

    /// A system (or developer) message inserted into the model's context.

When using along with `previous_response_id`, the instructions from a previous
response will not be carried over to the next response. This makes it simple
to swap out system (or developer) messages in new responses.
    public let instructions: String

    /// An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
    public let maxOutputTokens: Int?

    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
    public let maxToolCalls: Int?

    /// Set of 16 key-value pairs that can be attached to an object. This can be
useful for storing additional information about the object in a structured
format, and querying for objects via API or the dashboard.

Keys are strings with a maximum length of 64 characters. Values are strings
with a maximum length of 512 characters.
    public let metadata: [String: Any]

    /// The model used to generate this response.
    public let model: String

    /// The object type of this resource - always set to `response`.
    public let object: String

    /// An array of content items generated by the model.

- The length and order of items in the `output` array is dependent
  on the model's response.
- Rather than accessing the first item in the `output` array and
  assuming it's an `assistant` message with the content generated by
  the model, you might consider using the `output_text` property where
  supported in SDKs.
    public let output: [GeneratedItemResource]

    /// SDK-only convenience property that contains the aggregated text output
from all `output_text` items in the `output` array, if any are present.
Supported in the Python and JavaScript SDKs.
    public let outputText: String?

    /// Whether to allow the model to run tool calls in parallel.
    public let parallelToolCalls: Bool

    /// The unique ID of the previous response to the model. Use this to
create multi-turn conversations. Learn more about
[conversation state](/docs/guides/conversation-state).
    public let previousResponseId: String?

    public let prompt: [String: Any]?

    public let reasoning: [String: Any]?

    /// The status of the response generation. One of `completed`, `failed`,
`in_progress`, `cancelled`, `queued`, or `incomplete`.
    public let status: String?

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
We generally recommend altering this or `top_p` but not both.
    public let temperature: Float

    /// Configuration options for a text response from the model. Can be plain
text or structured JSON data. Learn more:
- [Text inputs and outputs](/docs/guides/text)
- [Structured Outputs](/docs/guides/structured-outputs)
    public let text: Generatedtext?

    /// How the model should select which tool (or tools) to use when generating
a response. See the `tools` parameter to see how to specify which tools
the model can call.
    public let toolChoice: String?

    /// An array of tools the model may call while generating a response. You
can specify which tool to use by setting the `tool_choice` parameter.

The two categories of tools you can provide the model are:

- **Built-in tools**: Tools that are provided by OpenAI that extend the
  model's capabilities, like [web search](/docs/guides/tools-web-search)
  or [file search](/docs/guides/tools-file-search). Learn more about
  [built-in tools](/docs/guides/tools).
- **Function calls (custom tools)**: Functions that are defined by you,
  enabling the model to call your own code. Learn more about
  [function calling](/docs/guides/function-calling).
    public let tools: [GeneratedTool]?

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    public let topLogprobs: Int?

    /// An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability
mass. So 0.1 means only the tokens comprising the top 10% probability mass
are considered.

We generally recommend altering this or `temperature` but not both.
    public let topP: Float

    /// The truncation strategy to use for the model response.
- `auto`: If the context of this response and previous ones exceeds
  the model's context window size, the model will truncate the
  response to fit the context window by dropping input items in the
  middle of the conversation.
- `disabled` (default): If a model response will exceed the context window
  size for a model, the request will fail with a 400 error.
    public let truncation: String?

    public let usage: GeneratedResponseUsage?

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
    public let user: String

    private enum CodingKeys: String, CodingKey {
        case background
        case createdAt = "created_at"
        case error
        case id
        case incompleteDetails = "incomplete_details"
        case instructions
        case maxOutputTokens = "max_output_tokens"
        case maxToolCalls = "max_tool_calls"
        case metadata
        case model
        case object
        case output
        case outputText = "output_text"
        case parallelToolCalls = "parallel_tool_calls"
        case previousResponseId = "previous_response_id"
        case prompt
        case reasoning
        case status
        case temperature
        case text
        case toolChoice = "tool_choice"
        case tools
        case topLogprobs = "top_logprobs"
        case topP = "top_p"
        case truncation
        case usage
        case user
    }

}

/// Generated model for OpenAI.CreateEmbeddingResponse
public struct GeneratedEmbeddingResponse: Codable, Equatable {
    /// The list of embeddings generated by the model.
    public let data: [GeneratedEmbedding]

    /// The name of the model used to generate the embedding.
    public let model: String

    /// The object type, which is always "list".
    public let object: String

    /// The usage information for the request.
    public let usage: Generatedusage

}

/// Generated model for OpenAI.DeleteFileResponse
public struct GeneratedDeleteFileResponse: Codable, Equatable {
    public let deleted: Bool

    public let id: String

    public let object: String

}

/// Generated model for OpenAI.Embedding
public struct GeneratedEmbedding: Codable, Equatable {
    /// The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the [embedding guide](/docs/guides/embeddings).
    public let embedding: String

    /// The index of the embedding in the list of embeddings.
    public let index: Int

    /// The object type, which is always "embedding".
    public let object: String

}

/// Generated model for OpenAI.ImplicitUserMessage
public struct GeneratedImplicitUserMessage: Codable, Equatable {
    public let content: String

}

/// Generated model for OpenAI.ItemContent
public struct GeneratedItemContent: Codable, Equatable {
    public let type: GeneratedItemContentType

}

/// Generated model for OpenAI.ItemParam
public struct GeneratedItemParam: Codable, Equatable {
    public let type: GeneratedItemType

}

/// Generated model for OpenAI.ItemResource
public struct GeneratedItemResource: Codable, Equatable {
    public let id: String

    public let type: GeneratedItemType

}

/// Generated model for OpenAI.Prompt
public struct GeneratedPrompt: Codable, Equatable {
    /// The unique identifier of the prompt template to use.
    public let id: String

    public let variables: [String: Any]?

    /// Optional version of the prompt template.
    public let version: String?

}

/// Generated model for OpenAI.Reasoning
public struct GeneratedReasoning: Codable, Equatable {
    public let effort: String?

    /// **Deprecated:** use `summary` instead.

A summary of the reasoning performed by the model. This can be
useful for debugging and understanding the model's reasoning process.
One of `auto`, `concise`, or `detailed`.
    public let generateSummary: String?

    /// A summary of the reasoning performed by the model. This can be
useful for debugging and understanding the model's reasoning process.
One of `auto`, `concise`, or `detailed`.
    public let summary: String?

    private enum CodingKeys: String, CodingKey {
        case effort
        case generateSummary = "generate_summary"
        case summary
    }

}

/// Generated model for OpenAI.ResponseError
public struct GeneratedResponseError: Codable, Equatable {
    public let code: GeneratedResponseErrorCode

    /// A human-readable description of the error.
    public let message: String

}

/// Generated model for OpenAI.ResponseItemList
public struct GeneratedResponseItemList: Codable, Equatable {
    /// A list of items used to generate this response.
    public let data: [GeneratedItemResource]

    /// The ID of the first item in the list.
    public let firstId: String

    /// Whether there are more items available.
    public let hasMore: Bool

    /// The ID of the last item in the list.
    public let lastId: String

    /// The type of object returned, must be `list`.
    public let object: String

    private enum CodingKeys: String, CodingKey {
        case data
        case firstId = "first_id"
        case hasMore = "has_more"
        case lastId = "last_id"
        case object
    }

}

/// Generated model for OpenAI.ResponsePromptVariables
public struct GeneratedResponsePromptVariables: Codable, Equatable {
}

/// Generated model for OpenAI.ResponseStreamEvent
public struct GeneratedResponseStreamEvent: Codable, Equatable {
    /// The sequence number for this event.
    public let sequenceNumber: Int

    public let type: GeneratedResponseStreamEventType

    private enum CodingKeys: String, CodingKey {
        case sequenceNumber = "sequence_number"
        case type
    }

}

/// Generated model for OpenAI.ResponseTextFormatConfiguration
public struct GeneratedResponseTextFormatConfiguration: Codable, Equatable {
    public let type: GeneratedResponseTextFormatConfigurationType

}

/// Generated model for OpenAI.ResponseUsage
public struct GeneratedResponseUsage: Codable, Equatable {
    /// The number of input tokens.
    public let inputTokens: Int

    /// A detailed breakdown of the input tokens.
    public let inputTokensDetails: Generatedinput_tokens_details

    /// The number of output tokens.
    public let outputTokens: Int

    /// A detailed breakdown of the output tokens.
    public let outputTokensDetails: Generatedoutput_tokens_details

    /// The total number of tokens used.
    public let totalTokens: Int

    private enum CodingKeys: String, CodingKey {
        case inputTokens = "input_tokens"
        case inputTokensDetails = "input_tokens_details"
        case outputTokens = "output_tokens"
        case outputTokensDetails = "output_tokens_details"
        case totalTokens = "total_tokens"
    }

}

/// Generated model for OpenAI.Tool
public struct GeneratedTool: Codable, Equatable {
    public let type: GeneratedToolType

}

/// Generated model for OpenAI.ToolChoiceObject
public struct GeneratedToolChoiceObject: Codable, Equatable {
    public let type: GeneratedToolChoiceObjectType

}
